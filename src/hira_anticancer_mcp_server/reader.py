"""
ê±´ê°•ë³´í—˜ì‹¬ì‚¬í‰ê°€ì›(HIRA) í•­ì•”í™”í•™ìš”ë²• íŒŒì¼ ë¦¬ë”.

Excel(í—ˆê°€ì´ˆê³¼_í•­ì•”ìš”ë²•)ê³¼ PDF(í•­ì•”í™”í•™ìš”ë²•_ê³µê³ ì „ë¬¸)ë¥¼ íŒŒì‹±í•˜ì—¬
MCP TextContent / ImageContent í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

ì „ëµ:
  - Excel: openpyxl text extraction (ë¨¸ì§€ì…€ forward-fill, data_only=True)
  - PDF:   í•˜ì´ë¸Œë¦¬ë“œ (í…ìŠ¤íŠ¸ ì „ìš© â†’ pdfplumber, í…Œì´ë¸” í¬í•¨ â†’ PyMuPDF ImageContent)
"""

from __future__ import annotations

import base64
import io
import logging
from pathlib import Path
from typing import Any

from mcp.types import ImageContent, TextContent

logger = logging.getLogger("hira-mcp-reader")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Excel ë¦¬ë” (openpyxl)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ì„ í˜¸ ì‹œíŠ¸ í‚¤ì›Œë“œ (sheet ë¯¸ì§€ì • ì‹œ ì´ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ì‹œíŠ¸ë¥¼ ìš°ì„  ì„ íƒ)
_PREFERRED_SHEET_KEYWORDS = ["ì¸ì •", "ìš©ë²•ìš©ëŸ‰"]

# í—¤ë” í–‰ íŒë³„ìš© í‚¤ì›Œë“œ (ì´ ì¤‘ 2ê°œ ì´ìƒ í¬í•¨ ì‹œ í—¤ë”ë¡œ ê°„ì£¼)
_HEADER_KEYWORDS = ["ìš”ë²•ì½”ë“œ", "ì•”ì¢…", "í•­ì•”í™”í•™ìš”ë²•", "íˆ¬ì—¬ëŒ€ìƒ", "íˆ¬ì—¬ë‹¨ê³„",
                    "ì—°ë²ˆ", "êµ¬ë¶„", "ì ì‘ì¦", "ì•½ì œ", "ì„±ë¶„ëª…"]


def read_excel(
    filepath: Path,
    *,
    sheet: str | None = None,
    cancer_type: str | None = None,
    max_rows: int = 200,
) -> list[TextContent]:
    """
    Excel íŒŒì¼ì„ ì½ì–´ Markdown í…Œì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        filepath: .xlsx íŒŒì¼ ê²½ë¡œ
        sheet: ì‹œíŠ¸ ì´ë¦„ (Noneì´ë©´ ìë™ ì„ íƒ)
        cancer_type: ì•”ì¢… í•„í„° (ì˜ˆ: "ë‚œì†Œì•”", "ìê¶ê²½ë¶€ì•”")
        max_rows: ìµœëŒ€ ë°˜í™˜ í–‰ ìˆ˜ (í† í° ì œí•œ ë°©ì§€)

    Returns:
        list[TextContent] â€” Markdown í…Œì´ë¸” + ìš”ì•½ ì •ë³´
    """
    import openpyxl

    wb = openpyxl.load_workbook(str(filepath), data_only=True, read_only=False)

    if sheet:
        if sheet not in wb.sheetnames:
            return [TextContent(
                type="text",
                text=f"âš ï¸ ì‹œíŠ¸ '{sheet}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                     f"ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œíŠ¸: {', '.join(wb.sheetnames)}"
            )]
        ws = wb[sheet]
    else:
        ws = _select_preferred_sheet(wb)

    # â”€â”€ ë¨¸ì§€ì…€ forward-fill ë§µ êµ¬ì¶• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    merge_map: dict[tuple[int, int], Any] = {}
    for merged_range in ws.merged_cells.ranges:
        top_left_value = ws.cell(
            row=merged_range.min_row,
            column=merged_range.min_col,
        ).value
        for row in range(merged_range.min_row, merged_range.max_row + 1):
            for col in range(merged_range.min_col, merged_range.max_col + 1):
                merge_map[(row, col)] = top_left_value

    # â”€â”€ ë°ì´í„° ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_rows: list[list[str]] = []
    for row_idx, row in enumerate(ws.iter_rows(values_only=False), start=1):
        cells: list[str] = []
        for cell in row:
            coord = (cell.row, cell.column)
            if coord in merge_map:
                val = merge_map[coord]
            else:
                val = cell.value
            cells.append(str(val).strip() if val is not None else "")
        all_rows.append(cells)

    sheet_title = ws.title
    sheet_names = wb.sheetnames
    wb.close()

    if not all_rows:
        return [TextContent(type="text", text="âš ï¸ ì‹œíŠ¸ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")]

    # â”€â”€ í—¤ë” ê°ì§€ (í‚¤ì›Œë“œ ê¸°ë°˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    header_idx = _find_header_row(all_rows)

    headers = all_rows[header_idx]
    data_rows = all_rows[header_idx + 1:]

    # ë¹ˆ í–‰ ì œê±° (ëª¨ë“  ì…€ì´ ë¹„ì–´ìˆëŠ” í–‰)
    data_rows = [row for row in data_rows if any(c for c in row)]

    # â”€â”€ ì•”ì¢… í•„í„° ì ìš© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if cancer_type:
        cancer_col_idx = _find_cancer_column(headers)
        if cancer_col_idx is not None:
            data_rows = [
                row for row in data_rows
                if cancer_type in row[cancer_col_idx]
            ]

    total_count = len(data_rows)
    truncated = total_count > max_rows
    data_rows = data_rows[:max_rows]

    # â”€â”€ Markdown í…Œì´ë¸” ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    md_lines = _to_markdown_table(headers, data_rows)

    # â”€â”€ ìš”ì•½ ì •ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    summary_parts = [
        f"ğŸ“Š ì‹œíŠ¸: {sheet_title}",
        f"ğŸ“ ì „ì²´ í–‰: {total_count}í–‰",
    ]
    if cancer_type:
        summary_parts.append(f"ğŸ” í•„í„°: '{cancer_type}'")
    if truncated:
        summary_parts.append(f"âš ï¸ {max_rows}í–‰ê¹Œì§€ë§Œ í‘œì‹œ (ì „ì²´ {total_count}í–‰)")

    summary = " | ".join(summary_parts)
    sheets_info = f"ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œíŠ¸: {', '.join(sheet_names)}"

    # â”€â”€ ê²€í† ì¤‘/ë¶ˆìŠ¹ì¸ ì‹œíŠ¸ ê²½ê³  ë©”ì‹œì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    warning = ""
    if "ê²€í† ì¤‘" in sheet_title:
        warning = (
            "\n\nğŸš¨ **ì£¼ì˜**: ì´ ì‹œíŠ¸ì˜ í•­ì•”ìš”ë²•ì€ í˜„ì¬ **ê²€í†  ì¤‘**ì…ë‹ˆë‹¤. "
            "ì•„ì§ ê±´ê°•ë³´í—˜ ê¸‰ì—¬ë¡œ ìŠ¹ì¸ë˜ì§€ ì•Šì•˜ìœ¼ë©°, í–¥í›„ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
            "ìŠ¹ì¸ëœ ìš”ë²•ì€ 'ì¸ì •ë˜ê³  ìˆëŠ” í—ˆê°€ì´ˆê³¼ í•­ì•”ìš”ë²•(ìš©ë²•ìš©ëŸ‰í¬í•¨)' ì‹œíŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        )
    elif "ë¶ˆìŠ¹ì¸" in sheet_title:
        warning = (
            "\n\nğŸš¨ **ì£¼ì˜**: ì´ ì‹œíŠ¸ì˜ í•­ì•”ìš”ë²•ì€ **ìŠ¹ì¸ ê±°ë¶€(ë¶ˆìŠ¹ì¸)**ë˜ì—ˆìŠµë‹ˆë‹¤. "
            "ê±´ê°•ë³´í—˜ ê¸‰ì—¬ë¡œ ì¸ì •ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
            "ìŠ¹ì¸ëœ ìš”ë²•ì€ 'ì¸ì •ë˜ê³  ìˆëŠ” í—ˆê°€ì´ˆê³¼ í•­ì•”ìš”ë²•(ìš©ë²•ìš©ëŸ‰í¬í•¨)' ì‹œíŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        )

    return [TextContent(type="text", text=f"{summary}\n{sheets_info}{warning}\n\n{md_lines}")]


def _select_preferred_sheet(wb):
    """ì„ í˜¸ ì‹œíŠ¸ë¥¼ ìë™ ì„ íƒí•©ë‹ˆë‹¤. í‚¤ì›Œë“œ ë§¤ì¹­ â†’ í™œì„± ì‹œíŠ¸ ìˆœ."""
    for name in wb.sheetnames:
        if all(kw in name for kw in _PREFERRED_SHEET_KEYWORDS):
            logger.info(f"ì„ í˜¸ ì‹œíŠ¸ ìë™ ì„ íƒ: {name}")
            return wb[name]
    return wb.active


def _find_header_row(all_rows: list[list[str]]) -> int:
    """í—¤ë” í‚¤ì›Œë“œê°€ í¬í•¨ëœ í–‰ì„ ì°¾ìŠµë‹ˆë‹¤. ì—†ìœ¼ë©´ ì²« ë¹„ì–´ìˆì§€ ì•Šì€ í–‰."""
    for i, row in enumerate(all_rows):
        row_text = " ".join(row).lower()
        matches = sum(1 for kw in _HEADER_KEYWORDS if kw in row_text)
        if matches >= 2:
            return i

    # fallback: ì²« ë²ˆì§¸ ë¹„ì–´ìˆì§€ ì•Šì€ í–‰
    for i, row in enumerate(all_rows):
        if any(c for c in row):
            return i
    return 0


def _find_cancer_column(headers: list[str]) -> int | None:
    """í—¤ë”ì—ì„œ ì•”ì¢… ê´€ë ¨ ì»¬ëŸ¼ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    cancer_keywords = ["ì•”ì¢…", "cancer", "ì§ˆí™˜", "ì ì‘ì¦", "ì§„ë‹¨", "ì•” ì¢…"]
    for idx, h in enumerate(headers):
        h_lower = h.lower()
        if any(kw in h_lower for kw in cancer_keywords):
            return idx
    # fallback: "íˆ¬ì—¬ëŒ€ìƒ" ì»¬ëŸ¼ (ì•”ì¢…ëª…ì´ íˆ¬ì—¬ëŒ€ìƒì— í¬í•¨ë˜ëŠ” ê²½ìš°ë„ ìˆìŒ)
    for idx, h in enumerate(headers):
        if "íˆ¬ì—¬ëŒ€ìƒ" in h:
            return idx
    return None


def _to_markdown_table(headers: list[str], rows: list[list[str]]) -> str:
    """í—¤ë”ì™€ ë°ì´í„° í–‰ì„ Markdown í…Œì´ë¸” ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if not headers:
        return "(ë¹ˆ í…Œì´ë¸”)"

    # ì—´ ìˆ˜ í†µì¼
    n_cols = len(headers)

    # ê¸´ ì…€ ë‚´ìš© ì¶•ì•½ (300ì ì´ˆê³¼ ì‹œ)
    def _trunc(s: str, limit: int = 300) -> str:
        return s[:limit] + "â€¦" if len(s) > limit else s

    header_line = "| " + " | ".join(_trunc(h) for h in headers) + " |"
    sep_line = "| " + " | ".join("---" for _ in headers) + " |"

    data_lines = []
    for row in rows:
        # ì—´ ìˆ˜ê°€ í—¤ë”ë³´ë‹¤ ì ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
        padded = row[:n_cols] + [""] * max(0, n_cols - len(row))
        line = "| " + " | ".join(_trunc(c) for c in padded) + " |"
        data_lines.append(line)

    return "\n".join([header_line, sep_line] + data_lines)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PDF ë¦¬ë” (pdfplumber + PyMuPDF í•˜ì´ë¸Œë¦¬ë“œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# í˜ì´ì§€ íƒ€ì… ê°ì§€ ìƒìˆ˜
_TABLE_THRESHOLD = 1  # pdfplumberê°€ Nê°œ ì´ìƒ í…Œì´ë¸” ê°ì§€ ì‹œ â†’ ì´ë¯¸ì§€ ë Œë”ë§
_MAX_PAGES_PER_CALL = 50  # í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ (í† í° ì œí•œ ë°©ì§€)
_IMAGE_DPI = 150  # ImageContent í•´ìƒë„
_MAX_IMAGE_PAGES = 5  # ì´ë¯¸ì§€ ë Œë”ë§ ìµœëŒ€ í˜ì´ì§€ (1MB ì œí•œ ë°©ì§€)

# ì•”ì¢… ì˜í•œ ë§¤í•‘ (ê²€ìƒ‰ìš©)
_CANCER_ALIASES: dict[str, list[str]] = {
    "ì†Œì„¸í¬íì•”": ["small cell lung", "sclc"],
    "ë¹„ì†Œì„¸í¬íì•”": ["non-small cell lung", "nsclc"],
    "ìœ„ì•”": ["gastric", "stomach"],
    "ì‹ë„ì•”": ["esophageal", "esophagus"],
    "ê°‘ìƒì„ ì•”": ["thyroid"],
    "ì·Œì¥ì•”": ["pancreatic", "pancreas"],
    "ê°„ì•”": ["hepatocellular", "liver", "hcc"],
    "ë‹´ë„ì•”": ["biliary", "cholangiocarcinoma"],
    "ì§ê²°ì¥ì•”": ["colorectal", "colon", "rectal", "crc"],
    "ìœ ë°©ì•”": ["breast"],
    "ë‚œì†Œì•”": ["ovarian", "ovary"],
    "ë‚œê´€ì•”": ["fallopian"],
    "ìê¶ê²½ë¶€ì•”": ["cervical", "cervix"],
    "ìê¶ì•”": ["uterine", "endometrial"],
    "ìê¶ë‚´ë§‰ì•”": ["endometrial", "endometrium"],
    "ì‹ ì¥ì•”": ["renal", "kidney", "rcc"],
    "ìš”ë¡œìƒí”¼ì•”": ["urothelial", "bladder"],
    "ì „ë¦½ì„ ì•”": ["prostate"],
    "ë‘ê²½ë¶€ì•”": ["head and neck", "head & neck"],
    "ì‹ ê²½ë‚´ë¶„ë¹„ì•”": ["neuroendocrine", "net"],
    "ë©”ë¥´ì¼ˆì„¸í¬ì•”": ["merkel"],
    "í”¼ë¶€ì•”": ["skin", "bcc", "scc"],
    "ê³¨ì•”": ["bone", "osteosarcoma"],
    "ì¤‘ì¶”ì‹ ê²½ê³„ì•”": ["cns", "brain", "glioma", "glioblastoma"],
    "ì•…ì„±í‘ìƒ‰ì¢…": ["melanoma"],
    "ì—°ì¡°ì§ìœ¡ì¢…": ["soft tissue sarcoma"],
    "íš¡ë¬¸ê·¼ìœ¡ì¢…": ["rhabdomyosarcoma"],
    "ìƒì‹ì„¸í¬ì¢…ì–‘": ["germ cell"],
    "ì‹ ê²½ëª¨ì„¸í¬ì¢…": ["neuroblastoma"],
    "ìœŒë¦„ì¦ˆì¢…ì–‘": ["wilms"],
    "ë§ë§‰ëª¨ì„¸í¬ì¢…": ["retinoblastoma"],
    "ë¹„í˜¸ì§€í‚¨ë¦¼í”„ì¢…": ["non-hodgkin", "nhl", "lymphoma"],
    "í˜¸ì§€í‚¨ë¦¼í”„ì¢…": ["hodgkin"],
    "ë‹¤ë°œê³¨ìˆ˜ì¢…": ["multiple myeloma", "myeloma"],
    "ê¸‰ì„±ê³¨ìˆ˜ì„±ë°±í˜ˆë³‘": ["aml", "acute myeloid"],
    "ê¸‰ì„±ì „ê³¨ìˆ˜êµ¬ì„±ë°±í˜ˆë³‘": ["apl", "promyelocytic"],
    "ë§Œì„±ê³¨ìˆ˜ì„±ë°±í˜ˆë³‘": ["cml", "chronic myeloid"],
    "ê¸‰ì„±ë¦¼í”„ëª¨êµ¬ë°±í˜ˆë³‘": ["all", "acute lymphoblastic"],
    "ë§Œì„±ë¦¼í”„êµ¬ì„±ë°±í˜ˆë³‘": ["cll", "chronic lymphocytic"],
    "ê³¨ìˆ˜í˜•ì„±ì´ìƒì¦í›„êµ°": ["mds", "myelodysplastic"],
}

# PDF ì„¹ì…˜ë³„ í‚¤ì›Œë“œ ë§¤í•‘ (í•­ì•”í™”í•™ìš”ë²• ê³µê³ ì „ë¬¸ êµ¬ì¡°)
PDF_SECTIONS: dict[str, list[str]] = {
    "ì¼ë°˜ì›ì¹™": ["ì¼ë°˜ì›ì¹™"],
    "ì•”ì¢…ë³„í•­ì•”ìš”ë²•": ["ì£¼ìš” ì•”ì¢…ë³„ í•­ì•”ìš”ë²•"],
    "í•­ì•”ë©´ì—­ìš”ë²•ì œ": ["í•­ì•”ë©´ì—­ìš”ë²•ì œ"],
    "í•­êµ¬í† ì œ": ["í•­êµ¬í† ì œ"],
    "ë³„í‘œ": ["ë³„í‘œ", "[ë³„í‘œ"],
    "ë¶€ë¡": ["ë¶€ë¡", "ë¶€í‘œ"],
}


def read_pdf(
    filepath: Path,
    *,
    pages: str | None = None,
    section: str | None = None,
    cancer_type: str | None = None,
    search: str | None = None,
    text_only: bool = False,
) -> list[TextContent | ImageContent]:
    """
    PDFë¥¼ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì½ìŠµë‹ˆë‹¤.

    Args:
        filepath: .pdf íŒŒì¼ ê²½ë¡œ
        pages: í˜ì´ì§€ ë²”ìœ„ (ì˜ˆ: "1-10", "5", "1,3,7-10"). Noneì´ë©´ ì²˜ìŒ 50p.
        section: ì„¹ì…˜ í•„í„° (ì˜ˆ: "ì¼ë°˜ì›ì¹™", "ë³„í‘œ").
        cancer_type: ì•”ì¢…ëª… (ì˜ˆ: "ë‚œì†Œì•”", "ovarian"). TOCì—ì„œ í˜ì´ì§€ ë²”ìœ„ ìë™ íƒìƒ‰.
        search: í‚¤ì›Œë“œ ê²€ìƒ‰ (ì˜ˆ: ì•½ì œëª…, ì•”ì¢…ëª…). ë§¤ì¹­ í˜ì´ì§€ì™€ ì£¼ë³€ í…ìŠ¤íŠ¸ ë°˜í™˜.
        text_only: Trueì´ë©´ ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ (1MB ì œí•œ íšŒí”¼).

    Returns:
        list[TextContent | ImageContent] í˜¼í•© ë¦¬ìŠ¤íŠ¸
    """
    import fitz  # PyMuPDF
    import pdfplumber

    doc = fitz.open(str(filepath))
    total_pages = len(doc)

    # â”€â”€ í‚¤ì›Œë“œ ê²€ìƒ‰ ëª¨ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if search:
        doc.close()
        return _search_pdf(filepath, search, total_pages)

    # â”€â”€ í˜ì´ì§€ ë²”ìœ„ ê²°ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    range_label = None  # ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë²”ìœ„ ì„¤ëª…

    if cancer_type:
        toc, toc_page_idx = _parse_toc(filepath)
        page_indices, matched_name = _find_cancer_pages(toc, cancer_type, total_pages, filepath, toc_page_idx)
        if not page_indices:
            doc.close()
            available = ", ".join(e["name"] for e in toc) if toc else "(TOC íŒŒì‹± ì‹¤íŒ¨)"
            return [TextContent(
                type="text",
                text=f"âš ï¸ ì•”ì¢… '{cancer_type}'ì„ ëª©ì°¨ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                     f"ì‚¬ìš© ê°€ëŠ¥í•œ ì•”ì¢…: {available}"
            )]
        range_label = f"ğŸ” ì•”ì¢…: '{matched_name}'"
    elif section:
        toc, toc_page_idx = _parse_toc(filepath)
        page_indices = _find_section_pages_from_toc(toc, section, filepath, total_pages, toc_page_idx)
        if not page_indices:
            doc.close()
            return [TextContent(
                type="text",
                text=f"âš ï¸ ì„¹ì…˜ '{section}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                     f"ì‚¬ìš© ê°€ëŠ¥í•œ ì„¹ì…˜: {', '.join(PDF_SECTIONS.keys())}\n"
                     f"ì´ {total_pages}í˜ì´ì§€"
            )]
        range_label = f"ğŸ” ì„¹ì…˜: '{section}'"
    elif pages:
        page_indices = _parse_page_range(pages, total_pages)
    else:
        # ê¸°ë³¸: TOC í˜ì´ì§€ë¥¼ ë³´ì—¬ì¤Œ (ì‚¬ìš©ìê°€ íƒìƒ‰í•  ìˆ˜ ìˆë„ë¡)
        toc, _toc_idx = _parse_toc(filepath)
        if toc:
            doc.close()
            return _format_toc_response(filepath, toc, total_pages)
        page_indices = list(range(min(total_pages, _MAX_PAGES_PER_CALL)))

    # 50í˜ì´ì§€ ì œí•œ ì ìš©
    truncated = len(page_indices) > _MAX_PAGES_PER_CALL
    page_indices = page_indices[:_MAX_PAGES_PER_CALL]

    # â”€â”€ ì´ë¯¸ì§€ í˜ì´ì§€ ìˆ˜ ì œí•œ (1MB ë°©ì§€) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # text_onlyê°€ ì•„ë‹Œ ê²½ìš°ì—ë„ ì´ë¯¸ì§€ í˜ì´ì§€ ìˆ˜ë¥¼ ì œí•œ
    image_page_count = 0

    # â”€â”€ í˜ì´ì§€ë³„ íƒ€ì… ê°ì§€ + íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    results: list[TextContent | ImageContent] = []

    # ì‹œì‘ ë©”íƒ€ ì •ë³´
    meta = (
        f"ğŸ“„ PDF: {filepath.name} ({total_pages}p)\n"
        f"ğŸ“– í‘œì‹œ ë²”ìœ„: {_format_page_range(page_indices)} "
        f"({len(page_indices)}p)"
    )
    if truncated:
        meta += f"\nâš ï¸ {_MAX_PAGES_PER_CALL}p ì œí•œ ì ìš©ë¨"
    if range_label:
        meta += f"\n{range_label}"
    if text_only:
        meta += "\nğŸ“ í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë“œ"
    results.append(TextContent(type="text", text=meta))

    # pdfplumberë¡œ í…Œì´ë¸” ê°ì§€
    pdf_plumber = pdfplumber.open(str(filepath))

    text_buffer: list[str] = []  # ì—°ì† í…ìŠ¤íŠ¸ í˜ì´ì§€ ë²„í¼

    for page_idx in page_indices:
        page_num = page_idx + 1  # 1-indexed

        try:
            plumber_page = pdf_plumber.pages[page_idx]
        except IndexError:
            continue

        # text_only ëª¨ë“œì´ë©´ í•­ìƒ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if text_only:
            text = _extract_text_safe(plumber_page, page_num)
            text_buffer.append(text)
            continue

        # pdfplumberë¡œ í…Œì´ë¸” ê°ì§€
        try:
            tables = plumber_page.find_tables()
            has_tables = len(tables) >= _TABLE_THRESHOLD
        except Exception:
            has_tables = False

        if has_tables and image_page_count < _MAX_IMAGE_PAGES:
            # í…ìŠ¤íŠ¸ ë²„í¼ê°€ ìˆìœ¼ë©´ ë¨¼ì € flush
            if text_buffer:
                results.append(TextContent(
                    type="text", text="\n\n".join(text_buffer)
                ))
                text_buffer.clear()

            # í…Œì´ë¸” í˜ì´ì§€ â†’ ì´ë¯¸ì§€ ë Œë”ë§ (PyMuPDF)
            try:
                fitz_page = doc[page_idx]
                mat = fitz.Matrix(_IMAGE_DPI / 72, _IMAGE_DPI / 72)
                pix = fitz_page.get_pixmap(matrix=mat)
                png_bytes = pix.tobytes("png")

                b64_data = base64.b64encode(png_bytes).decode("ascii")

                results.append(TextContent(
                    type="text",
                    text=f"--- ğŸ“Š p.{page_num} (í…Œì´ë¸” í¬í•¨ â†’ ì´ë¯¸ì§€) ---"
                ))
                results.append(ImageContent(
                    type="image",
                    data=b64_data,
                    mimeType="image/png",
                ))
                image_page_count += 1
            except Exception as e:
                logger.warning(f"í˜ì´ì§€ {page_num} ì´ë¯¸ì§€ ë Œë”ë§ ì‹¤íŒ¨: {e}")
                text = _extract_text_safe(plumber_page, page_num)
                text_buffer.append(text)
        else:
            # í…ìŠ¤íŠ¸ ì „ìš© í˜ì´ì§€ ë˜ëŠ” ì´ë¯¸ì§€ ì œí•œ ì´ˆê³¼
            if has_tables and image_page_count >= _MAX_IMAGE_PAGES:
                text = _extract_text_safe(plumber_page, page_num)
                text_buffer.append(
                    f"--- p.{page_num} (í…Œì´ë¸” í¬í•¨, ì´ë¯¸ì§€ ì œí•œ ì´ˆê³¼ â†’ í…ìŠ¤íŠ¸) ---\n"
                    + text.split("\n", 1)[-1] if "\n" in text else text
                )
            else:
                text = _extract_text_safe(plumber_page, page_num)
                text_buffer.append(text)

    # ë‚¨ì€ í…ìŠ¤íŠ¸ ë²„í¼ flush
    if text_buffer:
        results.append(TextContent(
            type="text", text="\n\n".join(text_buffer)
        ))

    if image_page_count >= _MAX_IMAGE_PAGES:
        results.append(TextContent(
            type="text",
            text=f"\nâš ï¸ ì´ë¯¸ì§€ ë Œë”ë§ {_MAX_IMAGE_PAGES}p ì œí•œ ë„ë‹¬. "
                 f"ë‚˜ë¨¸ì§€ í…Œì´ë¸” í˜ì´ì§€ëŠ” í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜ë¨. "
                 f"text_only=trueë¡œ ì „ì²´ í…ìŠ¤íŠ¸ ì¡°íšŒ ê°€ëŠ¥."
        ))

    pdf_plumber.close()
    doc.close()

    return results


def _extract_text_safe(plumber_page, page_num: int) -> str:
    """pdfplumber í˜ì´ì§€ì—ì„œ ì•ˆì „í•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        text = plumber_page.extract_text() or ""
        if text.strip():
            return f"--- p.{page_num} ---\n{text.strip()}"
        else:
            return f"--- p.{page_num} (ë¹ˆ í˜ì´ì§€) ---"
    except Exception as e:
        return f"--- p.{page_num} (ì¶”ì¶œ ì‹¤íŒ¨: {e}) ---"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PDF TOC íŒŒì‹± (ëª©ì°¨ì—ì„œ ì•”ì¢…â†’í˜ì´ì§€ ë§¤í•‘ ì¶”ì¶œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import re

# í•­ëª© ì‹œì‘ íŒ¨í„´: "ìˆ«ì. " ë˜ëŠ” "ìˆ«ì-ìˆ«ì. "
_TOC_ENTRY_START = re.compile(r"(\d+(?:-\d+)?)\.\s")

# ì„¹ì…˜ ë ˆë²¨ íŒ¨í„´: "â–¡ ì„¹ì…˜ëª…Â·Â·Â·ìˆ«ì"
_TOC_SECTION_PATTERN = re.compile(r"â–¡\s*(.+?)Â·+\s*(\d+)")


def _parse_toc_entries_from_line(line: str) -> list[dict]:
    """í•œ ì¤„ì—ì„œ TOC í•­ëª©ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤ (ë‘ ì»¬ëŸ¼ ëŒ€ì‘)."""
    entries = []
    # í•­ëª© ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
    starts = list(_TOC_ENTRY_START.finditer(line))
    for i, match in enumerate(starts):
        num = match.group(1)
        text_start = match.end()
        # ë‹¤ìŒ í•­ëª© ì‹œì‘ ë˜ëŠ” ì¤„ ëê¹Œì§€ê°€ ì´ í•­ëª©ì˜ í…ìŠ¤íŠ¸
        text_end = starts[i + 1].start() if i + 1 < len(starts) else len(line)
        segment = line[text_start:text_end].strip()

        # segmentì—ì„œ ì´ë¦„ê³¼ í˜ì´ì§€ ë²ˆí˜¸ ë¶„ë¦¬
        # íŒ¨í„´: "ì´ë¦„Â·Â·Â·Â·Â·ìˆ«ì" ë˜ëŠ” "ì´ë¦„ ìˆ«ì" (ë§ˆì§€ë§‰ ìˆ«ìê°€ í˜ì´ì§€)
        # ë¨¼ì € dot êµ¬ë¶„ ì‹œë„ (ì²« ë²ˆì§¸ Â·+ìˆ«ì ë§¤ì¹­ â€” ë¹„íƒìš•ì )
        dot_match = re.match(r"(.+?)Â·+(\d+)", segment)
        if dot_match:
            name = dot_match.group(1).strip()
            page = int(dot_match.group(2))
        else:
            # dot ì—†ëŠ” ê²½ìš°: ë§ˆì§€ë§‰ ìˆ«ìë¥¼ í˜ì´ì§€ë¡œ ì¶”ì¶œ
            num_match = re.search(r"\s(\d+)\s*$", segment)
            if num_match:
                name = segment[:num_match.start()].strip()
                page = int(num_match.group(1))
            else:
                continue  # íŒŒì‹± ì‹¤íŒ¨ â†’ ê±´ë„ˆëœ€

        name = re.sub(r"\s+", " ", name)
        if name and page > 0:
            entries.append({"num": num, "name": name, "page": page})

    return entries


def _parse_toc(filepath: Path) -> tuple[list[dict], int]:
    """
    PDF ëª©ì°¨ í˜ì´ì§€ë¥¼ íŒŒì‹±í•˜ì—¬ ì•”ì¢…ë³„ í˜ì´ì§€ ë§¤í•‘ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Returns:
        (entries, toc_page_idx) where entries is
        [{"num": "1", "name": "ì†Œì„¸í¬íì•”", "page": 16}, ...]
        í˜ì´ì§€ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬ë¨. toc_page_idxëŠ” ëª©ì°¨ í˜ì´ì§€ì˜ ì‹¤ì œ PDF ì¸ë±ìŠ¤.
    """
    import pdfplumber

    pdf = pdfplumber.open(str(filepath))
    toc_entries: list[dict] = []
    section_entries: list[dict] = []

    # ëª©ì°¨ í˜ì´ì§€ íƒìƒ‰ â€” ê°€ì¥ ë§ì€ í•­ëª©ì´ ìˆëŠ” í˜ì´ì§€ë¥¼ ì„ íƒ
    best_page_idx = -1
    best_count = 0
    for i in range(25, min(50, len(pdf.pages))):
        text = pdf.pages[i].extract_text() or ""
        if "ì¼ë°˜ì›ì¹™" in text and "ì•”ì¢…ë³„" in text:
            count = len(list(_TOC_ENTRY_START.finditer(text)))
            if count > best_count:
                best_count = count
                best_page_idx = i

    if best_page_idx >= 0:
        text = pdf.pages[best_page_idx].extract_text() or ""

        # ì„¹ì…˜ ë ˆë²¨ í•­ëª© ì¶”ì¶œ
        for match in _TOC_SECTION_PATTERN.finditer(text):
            name, page = match.group(1).strip(), int(match.group(2))
            section_entries.append({"name": name, "page": page})

        # ì¤„ ë‹¨ìœ„ë¡œ ì•”ì¢… í•­ëª© ì¶”ì¶œ
        for line in text.split("\n"):
            line = line.strip()
            if not line or line.startswith("â–¡") or line.startswith("ì•”í™˜ì"):
                continue
            entries = _parse_toc_entries_from_line(line)
            toc_entries.extend(entries)

    pdf.close()

    # í˜ì´ì§€ ë²ˆí˜¸ ìˆœ ì •ë ¬ (ë‘ ì»¬ëŸ¼ì´ ì„ì—¬ìˆìœ¼ë¯€ë¡œ)
    toc_entries.sort(key=lambda e: e["page"])

    # ì¤‘ë³µ ì œê±° (ê°™ì€ í˜ì´ì§€)
    seen = set()
    unique = []
    for entry in toc_entries:
        if entry["page"] not in seen:
            seen.add(entry["page"])
            unique.append(entry)
    toc_entries = unique

    # ê° í•­ëª©ì˜ end_page ê³„ì‚° (ë‹¤ìŒ í•­ëª©ì˜ ì‹œì‘ - 1)
    for i, entry in enumerate(toc_entries):
        if i + 1 < len(toc_entries):
            entry["end_page"] = toc_entries[i + 1]["page"] - 1
        else:
            # ë§ˆì§€ë§‰ ì•”ì¢… í•­ëª©: "í•­ì•”ë©´ì—­ìš”ë²•ì œ" ì„¹ì…˜ ì‹œì‘ ì „ê¹Œì§€
            next_section_page = None
            for sec in section_entries:
                if sec["page"] > entry["page"]:
                    next_section_page = sec["page"]
                    break
            entry["end_page"] = (next_section_page - 1) if next_section_page else entry["page"] + 10

    logger.info(f"TOC íŒŒì‹± ì™„ë£Œ: {len(toc_entries)}ê°œ í•­ëª©, TOC page idx={best_page_idx}")
    return toc_entries, best_page_idx


def _find_cancer_pages(
    toc: list[dict], cancer_type: str, total_pages: int,
    filepath: Path | None = None,
    toc_page_idx: int = -1,
) -> tuple[list[int], str]:
    """
    TOCì—ì„œ ì•”ì¢…ëª…ìœ¼ë¡œ í˜ì´ì§€ ë²”ìœ„ë¥¼ ì°¾ìŠµë‹ˆë‹¤. í¼ì§€ ë§¤ì¹­ ì§€ì›.

    Returns:
        (page_indices, matched_name) â€” ëª» ì°¾ìœ¼ë©´ ([], "")
    """
    query = cancer_type.lower().strip()

    def _resolve(entry: dict) -> tuple[list[int], str]:
        start, end = _toc_page_to_indices(entry, toc, total_pages, filepath, toc_page_idx)
        return list(range(start, end + 1)), entry["name"]

    # 1ë‹¨ê³„: ì •í™•í•œ í•œê¸€ ì´ë¦„ ë§¤ì¹­
    for entry in toc:
        if query in entry["name"]:
            return _resolve(entry)

    # 2ë‹¨ê³„: ì˜ë¬¸ ë³„ì¹­ ë§¤ì¹­
    for korean_name, aliases in _CANCER_ALIASES.items():
        if query in korean_name or any(alias in query for alias in aliases):
            for entry in toc:
                if korean_name in entry["name"]:
                    return _resolve(entry)

    # 3ë‹¨ê³„: ë¶€ë¶„ ë§¤ì¹­ (ê°€ì¥ ìœ ì‚¬í•œ í•­ëª©)
    for entry in toc:
        entry_lower = entry["name"].lower()
        if any(c in entry_lower for c in query if len(c) > 1):
            return _resolve(entry)

    return [], ""


_toc_offset_cache: dict[str, int] = {}


def _calc_toc_offset(
    filepath: Path, toc: list[dict], toc_page_idx: int = -1
) -> int:
    """
    TOC í˜ì´ì§€ ë²ˆí˜¸ì™€ ì‹¤ì œ PDF í˜ì´ì§€ì˜ ì˜¤í”„ì…‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

    ë°©ë²•: TOC ì§í›„ ì²« ì½˜í…ì¸  í˜ì´ì§€ì˜ í•˜ë‹¨ ì¸ì‡„ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì½ì–´ì„œ
    offset = pdf_idx - printed_number + 1 ë¡œ ê³„ì‚°.
    """
    cache_key = str(filepath)
    if cache_key in _toc_offset_cache:
        return _toc_offset_cache[cache_key]

    import pdfplumber

    pdf = pdfplumber.open(str(filepath))

    # ë°©ë²• 1: TOC ì§í›„ í˜ì´ì§€ì˜ footer ë²ˆí˜¸ë¡œ ì˜¤í”„ì…‹ ê³„ì‚°
    if toc_page_idx >= 0:
        for scan_idx in range(toc_page_idx + 1, min(toc_page_idx + 5, len(pdf.pages))):
            text = pdf.pages[scan_idx].extract_text() or ""
            lines = [ln.strip() for ln in text.strip().split("\n") if ln.strip()]
            if not lines:
                continue
            # footer: ë§ˆì§€ë§‰ ì¤„ì´ ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°
            last_line = lines[-1]
            footer_match = re.match(r"^(\d+)$", last_line)
            if footer_match:
                footer_num = int(footer_match.group(1))
                offset = scan_idx - footer_num + 1
                _toc_offset_cache[cache_key] = offset
                pdf.close()
                logger.info(
                    f"TOC ì˜¤í”„ì…‹ ê³„ì‚° (footer): {offset} "
                    f"(PDF idx={scan_idx}, footer={footer_num})"
                )
                return offset

    # ë°©ë²• 2 (fallback): "ì¼ë°˜ì›ì¹™" í…ìŠ¤íŠ¸ ìœ„ì¹˜ + TOC/section í•­ëª© ëŒ€ì¡°
    for i in range(30, min(50, len(pdf.pages))):
        text = (pdf.pages[i].extract_text() or "")[:500]
        if "ì¼ë°˜ì›ì¹™" in text:
            # footer ë²ˆí˜¸ í™•ì¸
            lines = [ln.strip() for ln in text.strip().split("\n") if ln.strip()]
            footer_match = re.match(r"^(\d+)$", lines[-1]) if lines else None
            if footer_match:
                footer_num = int(footer_match.group(1))
                offset = i - footer_num + 1
                _toc_offset_cache[cache_key] = offset
                pdf.close()
                logger.info(f"TOC ì˜¤í”„ì…‹ ê³„ì‚° (ì¼ë°˜ì›ì¹™ fallback): {offset}")
                return offset

    pdf.close()

    # ìµœí›„ fallback
    _toc_offset_cache[cache_key] = 33
    logger.warning("TOC ì˜¤í”„ì…‹ ê³„ì‚° ì‹¤íŒ¨, ê¸°ë³¸ê°’ 33 ì‚¬ìš©")
    return 33


def _toc_page_to_indices(
    entry: dict, toc: list[dict], total_pages: int,
    filepath: Path | None = None,
    toc_page_idx: int = -1,
) -> tuple[int, int]:
    """
    TOC í˜ì´ì§€ ë²ˆí˜¸(PDF ë‚´ë¶€ ë²ˆí˜¸)ë¥¼ 0-indexed í˜ì´ì§€ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    offset = _calc_toc_offset(filepath, toc, toc_page_idx) if filepath else 33

    start_idx = entry["page"] + offset - 1  # 0-indexed
    end_idx = entry["end_page"] + offset - 1

    # Â±2 í˜ì´ì§€ í¼ì§€ ê²€ì¦: ì•”ì¢…ëª…ì´ ì‹¤ì œ í•´ë‹¹ í˜ì´ì§€ì— ìˆëŠ”ì§€ í™•ì¸
    if filepath and entry.get("name"):
        start_idx = _verify_page_with_fuzzy(
            filepath, start_idx, entry["name"], total_pages
        )
        # endë„ ì¡°ì • (startì™€ì˜ ì°¨ì´ ìœ ì§€)
        page_span = entry["end_page"] - entry["page"]
        end_idx = start_idx + page_span

    # ë²”ìœ„ ê²€ì¦
    start_idx = max(0, min(start_idx, total_pages - 1))
    end_idx = max(start_idx, min(end_idx, total_pages - 1))

    return start_idx, end_idx


def _verify_page_with_fuzzy(
    filepath: Path, expected_idx: int, cancer_name: str, total_pages: int,
    search_range: int = 2,
) -> int:
    """
    ì˜ˆìƒ í˜ì´ì§€ Â±search_range ë²”ìœ„ì—ì„œ ì•”ì¢…ëª…ì„ ê²€ìƒ‰í•˜ì—¬ ì‹¤ì œ ì‹œì‘ í˜ì´ì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì°¾ì§€ ëª»í•˜ë©´ ì›ë˜ expected_idxë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    import pdfplumber

    # ì§§ì€ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: "ë‚œì†Œì•”/ë‚œê´€ì•”/ì¼ì°¨ë³µë§‰ì•”" â†’ ["ë‚œì†Œì•”", "ë‚œê´€ì•”"])
    name_parts = [p.strip() for p in cancer_name.replace("/", "|").split("|") if len(p.strip()) >= 2]
    if not name_parts:
        return expected_idx

    pdf = pdfplumber.open(str(filepath))
    try:
        # ì˜ˆìƒ í˜ì´ì§€ ë¨¼ì € í™•ì¸
        if 0 <= expected_idx < total_pages:
            text = (pdf.pages[expected_idx].extract_text() or "")[:500]
            if any(part in text for part in name_parts):
                return expected_idx

        # Â±search_range íƒìƒ‰
        for delta in range(1, search_range + 1):
            for candidate in [expected_idx + delta, expected_idx - delta]:
                if 0 <= candidate < total_pages:
                    text = (pdf.pages[candidate].extract_text() or "")[:500]
                    if any(part in text for part in name_parts):
                        logger.info(
                            f"í¼ì§€ ê²€ì¦: '{cancer_name}' í˜ì´ì§€ ì¡°ì • "
                            f"{expected_idx} â†’ {candidate}"
                        )
                        return candidate
    finally:
        pdf.close()

    return expected_idx


def _find_section_pages_from_toc(
    toc: list[dict], section: str, filepath: Path, total_pages: int,
    toc_page_idx: int = -1,
) -> list[int]:
    """TOC ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ í˜ì´ì§€ ë²”ìœ„ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ìŠ¤ìº” í´ë°±."""
    keywords = PDF_SECTIONS.get(section, [section])

    # TOCì—ì„œ ê²€ìƒ‰
    for entry in toc:
        if any(kw in entry["name"] for kw in keywords):
            start, end = _toc_page_to_indices(entry, toc, total_pages, filepath, toc_page_idx)
            return list(range(start, end + 1))

    # í´ë°±: í…ìŠ¤íŠ¸ ìŠ¤ìº”
    return _find_section_pages_by_scan(filepath, section, total_pages)


def _find_section_pages_by_scan(
    filepath: Path, section: str, total_pages: int
) -> list[int]:
    """PDF ì „ì²´ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì„¹ì…˜ í˜ì´ì§€ë¥¼ ì°¾ìŠµë‹ˆë‹¤ (í´ë°±)."""
    import pdfplumber

    keywords = PDF_SECTIONS.get(section, [section])
    pdf = pdfplumber.open(str(filepath))
    start_page = None

    for i, page in enumerate(pdf.pages):
        text = (page.extract_text() or "").strip()
        if not text:
            continue
        header = text[:500]
        if any(kw in header for kw in keywords):
            start_page = i
            break

    if start_page is None:
        pdf.close()
        return []

    # ë‹¤ìŒ ì„¹ì…˜ ì‹œì‘ì  íƒìƒ‰
    end_page = min(start_page + 50, total_pages - 1)
    other_keywords = []
    for sec_name, sec_kws in PDF_SECTIONS.items():
        if sec_name != section:
            other_keywords.extend(sec_kws)

    for i in range(start_page + 1, min(start_page + 100, total_pages)):
        text = (pdf.pages[i].extract_text() or "").strip()
        if any(kw in text[:500] for kw in other_keywords):
            end_page = i - 1
            break

    pdf.close()
    return list(range(start_page, end_page + 1))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PDF í‚¤ì›Œë“œ ê²€ìƒ‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_SEARCH_MAX_RESULTS = 20
_SEARCH_CONTEXT_CHARS = 200


def _search_pdf(
    filepath: Path, keyword: str, total_pages: int
) -> list[TextContent]:
    """PDF ì „ì²´ì—ì„œ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë§¤ì¹­ í˜ì´ì§€ì™€ ì£¼ë³€ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    import pdfplumber

    pdf = pdfplumber.open(str(filepath))
    matches: list[dict] = []
    keyword_lower = keyword.lower()

    for i, page in enumerate(pdf.pages):
        if len(matches) >= _SEARCH_MAX_RESULTS:
            break
        text = page.extract_text() or ""
        if keyword_lower in text.lower():
            # ë§¤ì¹­ ìœ„ì¹˜ì˜ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            idx = text.lower().index(keyword_lower)
            start = max(0, idx - _SEARCH_CONTEXT_CHARS)
            end = min(len(text), idx + len(keyword) + _SEARCH_CONTEXT_CHARS)
            context = text[start:end].strip()
            if start > 0:
                context = "â€¦" + context
            if end < len(text):
                context = context + "â€¦"
            matches.append({"page": i + 1, "context": context})

    pdf.close()

    if not matches:
        return [TextContent(
            type="text",
            text=f"ğŸ” '{keyword}' ê²€ìƒ‰ ê²°ê³¼: 0ê±´ (ì „ì²´ {total_pages}p ê²€ìƒ‰)\n"
                 "ë‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ ì˜ë¬¸/í•œê¸€ ë³€í˜•ì„ ì‹œë„í•´ë³´ì„¸ìš”."
        )]

    lines = [
        f"ğŸ” '{keyword}' ê²€ìƒ‰ ê²°ê³¼: {len(matches)}ê±´ "
        f"(ì „ì²´ {total_pages}p ê²€ìƒ‰)",
        "â”€" * 40,
    ]
    for m in matches:
        lines.append(f"\nğŸ“ p.{m['page']}:")
        lines.append(m["context"])

    lines.append("\nâ”€" * 40)
    lines.append(
        "ğŸ’¡ íŠ¹ì • í˜ì´ì§€ë¥¼ ìì„¸íˆ ë³´ë ¤ë©´ pages íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. "
        "ì˜ˆ: pages='" + ",".join(str(m["page"]) for m in matches[:5]) + "'"
    )

    return [TextContent(type="text", text="\n".join(lines))]


def _format_toc_response(
    filepath: Path, toc: list[dict], total_pages: int
) -> list[TextContent]:
    """TOCë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    lines = [
        f"ğŸ“„ PDF: {filepath.name} ({total_pages}p)",
        "",
        "ğŸ“‹ ëª©ì°¨ (cancer_type íŒŒë¼ë¯¸í„°ë¡œ ì•”ì¢…ë³„ ì¡°íšŒ ê°€ëŠ¥):",
        "â”€" * 50,
    ]
    for entry in toc:
        lines.append(f"  {entry['num']:>5}. {entry['name']:<20} â†’ p.{entry['page']}")

    lines.append("â”€" * 50)
    lines.append("")
    lines.append("ğŸ’¡ ì‚¬ìš©ë²•:")
    lines.append("  â€¢ cancer_type='ë‚œì†Œì•”' â†’ í•´ë‹¹ ì•”ì¢… í˜ì´ì§€ ìë™ ì¡°íšŒ")
    lines.append("  â€¢ search='trastuzumab' â†’ ì „ì²´ PDFì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰")
    lines.append("  â€¢ pages='64-68' â†’ íŠ¹ì • í˜ì´ì§€ ë²”ìœ„ ì§ì ‘ ì¡°íšŒ")
    lines.append("  â€¢ text_only=true â†’ ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ (ë„“ì€ ë²”ìœ„ ì¡°íšŒ)")

    return [TextContent(type="text", text="\n".join(lines))]


def _parse_page_range(pages_str: str, total: int) -> list[int]:
    """
    í˜ì´ì§€ ë²”ìœ„ ë¬¸ìì—´ì„ 0-indexed ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    ì§€ì› í˜•ì‹:
      - "5"       â†’ [4]
      - "1-10"    â†’ [0,1,...,9]
      - "1,3,7-10" â†’ [0,2,6,7,8,9]
    """
    result: list[int] = []
    for part in pages_str.split(","):
        part = part.strip()
        if "-" in part:
            start_s, end_s = part.split("-", 1)
            start = max(int(start_s.strip()) - 1, 0)
            end = min(int(end_s.strip()) - 1, total - 1)
            result.extend(range(start, end + 1))
        else:
            idx = int(part) - 1
            if 0 <= idx < total:
                result.append(idx)

    return sorted(set(result))


def _format_page_range(indices: list[int]) -> str:
    """0-indexed ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ í˜ì´ì§€ ë²”ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if not indices:
        return "(ì—†ìŒ)"

    # ì—°ì† êµ¬ê°„ íƒì§€
    ranges: list[str] = []
    start = indices[0]
    prev = indices[0]

    for i in indices[1:]:
        if i == prev + 1:
            prev = i
        else:
            if start == prev:
                ranges.append(f"p.{start + 1}")
            else:
                ranges.append(f"p.{start + 1}-{prev + 1}")
            start = i
            prev = i

    if start == prev:
        ranges.append(f"p.{start + 1}")
    else:
        ranges.append(f"p.{start + 1}-{prev + 1}")

    return ", ".join(ranges)
